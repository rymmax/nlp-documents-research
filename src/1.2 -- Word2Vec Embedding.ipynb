{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b097868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Embedding\n",
    "\n",
    "# import libraries we need...\n",
    "\n",
    "import glob\n",
    "import inflect\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors, Phrases\n",
    "from gensim.parsing.preprocessing import strip_numeric, strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_short, strip_punctuation\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from spherecluster import  VonMisesFisherMixture\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ae660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7826d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [] \n",
    "for i in glob.glob('./extracted_papers/*.txt'):\n",
    "    paper = open(i, encoding='utf-8')\n",
    "    articles.append(paper.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-unicode & newline characters\n",
    "\n",
    "a = ''.join([chr(n) for n in range(256)])\n",
    "unwanted = '[' + re.escape(''.join([n for n in a if ord(n) < 32 or ord(n) > 128])) + ']'\n",
    "\n",
    "cleaned_articles = list(map(lambda x : x.lower(),\n",
    "                           list(map(lambda x: re.sub(unwanted, ' ', x), articles))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cacb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted text formats (numeric, whitespace, punctuation, short words stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d975532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(s):\n",
    "\n",
    "    s = strip_numeric(strip_multiple_whitespaces(s))\n",
    "    s = strip_short(strip_punctuation(s), minsize = 2)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a047269",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = list(map(preprocess_text, cleaned_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c313a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all non-alphabetical charaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = []\n",
    "for i in cleaned_articles:\n",
    "    cleaned_sentences += list(map(lambda x: x, tokenize.sent_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + ['’','“', '‘', 'within', \n",
    "                                               'however','”','\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f8']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping stopwords and tokenization of sentence to words\n",
    "cleaned_sentences_w = list(map(lambda sentence: [w for w in tokenize.word_tokenize(sentence) if not w in stop_words], cleaned_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e95267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def singularize(sentence):\n",
    "    p = inflect.engine()\n",
    "    for i,word in enumerate(sentence):\n",
    "        if p.singular_noun(word):\n",
    "            sentence[i] = p.singular_noun(word)\n",
    "    return sentence\n",
    "        \n",
    "cleaned_sentences_w = list(map(singularize, cleaned_sentences_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb772067",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(cleaned_sentences_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(list(bigram_transformer[cleaned_sentences_w]),\n",
    "                 window = 5, min_count = 3, size = 200)\n",
    "model.train(cleaned_sentences_w,total_examples=len(cleaned_sentences_w),epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370cd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar ('derivative', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save(\"word_vectors.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23251a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ed7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(sentences):\n",
    "    \n",
    "    word_counts = Counter(itertools.chain(*sentences)) # Building of vocabulary\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()] # Mapping from index to word\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)} # Mapping from word to index\n",
    "    \n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d21378",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(itertools.chain(*list(bigram_transformer[cleaned_sentences_w])))\n",
    "\n",
    "# Here we rank words by importance\n",
    "vocabulary = {x: i for i, x in enumerate([x[0] for x in word_counts.most_common()])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb5511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3603fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"word_vectors.kv\"\n",
    "model = KeyedVectors.load(filename, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ebbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = model.wv.vectors \n",
    "vocab = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09b012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "linfnorm = np.linalg.norm(word_embedding, axis=1, ord=2)\n",
    "word_embedding_normalized = word_embedding / linfnorm[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(word_embedding_normalized.T, columns=vocab)\n",
    "words_df[['equity','stock', 'fixed_income', 'bond','real_estate','derivative', 'cds', 'swap', 'mortgage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26653e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25571616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords_str  = open('class keywords.txt', encoding='utf-8').read()\n",
    "class_keywords = {i.split(': ')[0]: i.split(': ')[1].split(', ') for i in class_keywords_str.split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25690a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords_supplied = {class_label: [np.array(words_df[word]) for word in words] \n",
    "                           for class_label, words in class_keywords.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bd92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_r = []\n",
    "mu_r = []\n",
    "\n",
    "for i in class_keywords_supplied.keys():\n",
    "    \n",
    "    vmF = VonMisesFisherMixture(n_clusters=1, n_jobs=10)\n",
    "    vmF.fit(np.vstack(class_keywords_supplied[i]))\n",
    "    \n",
    "    mu_r.append(vmF.cluster_centers_[0])\n",
    "    kappa_r.append(vmF.concentrations_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
