{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5212ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating of Dummy Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from gensim.models import  KeyedVectors\n",
    "from spherecluster import  VonMisesFisherMixture, sample_vMF\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1379c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"word_vectors.kv\"\n",
    "model = KeyedVectors.load(filename, mmap='r')\n",
    "\n",
    "word_embedding = np.array(model.wv.vectors)\n",
    "vocab = list(model.wv.vocab)\n",
    "\n",
    "linfnorm = np.linalg.norm(word_embedding, axis=1, ord=2)\n",
    "word_embedding = word_embedding / linfnorm[:,None]\n",
    "\n",
    "words_df = pd.DataFrame(word_embedding.T, columns=vocab)\n",
    "\n",
    "class_keywords_str  = open('class keywords.txt', encoding='utf-8').read()\n",
    "class_keywords = {i.split(': ')[0]: i.split(': ')[1].split(', ') for i in class_keywords_str.split('\\n')}\n",
    "\n",
    "class_keywords = {topic: [i[0] for i in model.wv.most_similar (topic, topn = 100)] for topic in class_keywords.keys()}\n",
    "class_keywords_supplied = {class_label: [np.array(words_df[word]) for word in words] \n",
    "                           for class_label, words in class_keywords.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26126d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vMFs = {}\n",
    "\n",
    "for i in class_keywords_supplied.keys():\n",
    "    keyword_mtx = np.vstack(class_keywords_supplied[i])\n",
    "    vmF = VonMisesFisherMixture(n_clusters=1, n_jobs=10, max_iter= 20)\n",
    "    vmF.fit(keyword_mtx)\n",
    "    topic_vMFs[i] = (vmF.cluster_centers_[0], vmF.concentrations_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d4af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.load('word_counts.npy').item()\n",
    "total_length = sum(word_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_distributions = {i : word_count/total_length for i, word_count in word_counts.items()}\n",
    "word_distributions = pd.DataFrame.from_records(word_distributions,index=[0])\n",
    "word_distributions = word_distributions[words_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d5259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611df35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of psuedo documents distribution given a topic\n",
    "\n",
    "def generateWordDistribution(alpha, word_distributions, top_n_keywords, words_df, topic):\n",
    "\n",
    "    mu, kappa = topic_vMFs[topic]\n",
    "    di = sample_vMF(mu, kappa, num_samples = 1)\n",
    "    \n",
    "    di_similarities = np.exp(np.dot(di, words_df.values).ravel())\n",
    "    ranked_index = np.argsort(di_similarities)[::-1]\n",
    "    \n",
    "    di_similarities[ranked_index[top_n_keywords:]] = 0\n",
    "    \n",
    "    # generate document distributions\n",
    "    keywords_distributions = di_similarities/np.sum(di_similarities)\n",
    "    background_words = word_distributions.values.ravel()\n",
    "    \n",
    "    document_distributions = (alpha* np.array(background_words)\n",
    "                                      + (1 - alpha)* keywords_distributions.ravel())\n",
    "    \n",
    "    return document_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adbe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0de05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePseudoLabels(alpha, word_distributions, topic):\n",
    "    \"\"\" Generates psuedo labels given a topic\n",
    "        Input: alpha - balancing parameter between background words and keywords\n",
    "               vocab - vocabulary lists\n",
    "               topic - topic keywords\n",
    "        Output: a vector similiar to one-hot, with the largest probabilities at the topic keyword\n",
    "    \"\"\"\n",
    "    # generate pseudo label\n",
    "    background_words = word_distributions.values.ravel()\n",
    "    label_vector = np.ones(len(background_words))*alpha/len(background_words)\n",
    "    label_vector[list(word_distributions).index(topic)] += 1 - alpha\n",
    "    return label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c31c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLabelledPseudoDocuments(alpha, doc_length, num_docs):\n",
    "    \"\"\" Generates psuedo documents given a topic\n",
    "        Input: alpha - balancing parameter between background words and keywords\n",
    "               doc_length - length of words in the pseudo document\n",
    "               num_docs - number of documents in a batch\n",
    "        Output: a tuple (pseudo docs, pseudo labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    topics = class_keywords_supplied.keys()\n",
    "    topic_docs = {}\n",
    "    for topic in topics:\n",
    "        pseudo_docs = []\n",
    "        pseudo_labels = []\n",
    "        for i in range(num_docs):\n",
    "        \n",
    "            document_distribution = generateWordDistribution(alpha, word_distributions, \n",
    "                                                         20, words_df, topic)\n",
    "            pseudo_docs.append(np.random.choice(len(document_distribution), size=doc_length, p=document_distribution)) \n",
    "            pseudo_labels.append(generatePseudoLabels(alpha, word_distributions, topic))\n",
    "        \n",
    "        topic_docs[topic] = (pseudo_docs, pseudo_labels)\n",
    "        \n",
    "    return topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceda526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_docs = generateLabelledPseudoDocuments(alpha = 0.3, doc_length = 1000, num_docs = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
