{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import inflect\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors, Phrases\n",
    "from gensim.parsing.preprocessing import strip_short,strip_punctuation,\\\n",
    "                                         strip_numeric, strip_multiple_whitespaces\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('./pseudo_docs.npy').item()\n",
    "labels = np.load('./pseudo_labels.npy').item()\n",
    "\n",
    "label_names = ['equity', 'fixed_income', 'derivatives', 'alternatives']\n",
    "training_x = np.vstack(list(map(lambda x: training_data[x], label_names)))\n",
    "training_y = np.vstack(list(map(lambda x: labels[x], label_names)))\n",
    "\n",
    "filename = \"./word vectors.kv\"\n",
    "word_vec = KeyedVectors.load(filename, mmap='r')\n",
    "word_embedding = np.array(word_vec.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f89b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(word_vec.wv.vocab)\n",
    "word_embedding_padded = np.vstack([np.zeros((1, word_embedding.shape[1])), word_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Data Tensor:', training_x.shape)\n",
    "print('Shape of Label Tensor:', training_y.shape)\n",
    "\n",
    "indices = np.arange(training_x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = training_x[indices]\n",
    "y_train = training_y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d3173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_embedding_padded.shape[0],\n",
    "                            word_embedding_padded.shape[1],\n",
    "                            weights=[word_embedding_padded],\n",
    "                            input_length=5000,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(5000,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)\n",
    "\n",
    "l_flat = Flatten()(l_pool3)\n",
    "\n",
    "l_dense_1 = Dense(128, activation='relu')(l_flat)\n",
    "l_dropout_1 = Dropout(0.25)(l_dense_1)\n",
    "\n",
    "l_dense_2 = Dense(128, activation='relu')(l_dropout_1)\n",
    "l_dropout_2 = Dropout(0.4)(l_dense_2)\n",
    "\n",
    "preds = Dense(4, activation='softmax')(l_dropout_2)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9381fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,epochs = 20, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('./model/model.json', 'w') as wf:\n",
    "    wf.write(model_json)\n",
    "    \n",
    "model.save('./model/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_index(file_path):\n",
    "    articles = []\n",
    "    labels = []\n",
    "    for i in glob.glob(file_path + '/*.txt'):\n",
    "        try:\n",
    "            paper = open(i, encoding='utf-8')\n",
    "            articles.append(paper.read())\n",
    "            labels.append(i.split('/')[-1].split('.')[0][5:])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x:x.lower(), articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r\"[^a-zA-Z0-9()_-]\", ' ', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r\"- \", \"\", x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bx.*?\\b', '', x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\by[^aeiou].*?\\b', '', x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\w*\\d\\w*\\s*', '', x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bmax\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bmin\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bsup\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\blim\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bexp\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\beqz\\b\\s*', '', x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\b(\\w+)\\s+\\1\\b\\s*', '', x), cleaned_articles))\n",
    "    \n",
    "    cleaned_articles = list(map(lambda x: ''.join(x.split('reference')[:-1])\n",
    "                                if x.find('reference') != -1 else x, cleaned_articles))\n",
    "    \n",
    "    def preprocess_text(s):\n",
    "        s = strip_multiple_whitespaces(s)\n",
    "        s = strip_punctuation(s)\n",
    "        s = strip_short(s, minsize = 3)\n",
    "        regex = re.compile('[^\\w]')\n",
    "        regex.sub('', s)\n",
    "        return s\n",
    "    cleaned_articles = list(map(preprocess_text, cleaned_articles))\n",
    "    cleaned_sentences = []\n",
    "    for i in cleaned_articles:\n",
    "        cleaned_sentences += list(map(lambda x: x, tokenize.sent_tokenize(i)))\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + ['within', 'however']) \n",
    "    \n",
    "    cleaned_sentences_w = list(map(lambda sentence: [w for w in tokenize.word_tokenize(sentence) if not w in stop_words], \n",
    "                              cleaned_sentences))\n",
    "    \n",
    "    bigram_transformer = Phrases(cleaned_sentences_w)\n",
    "    return list(bigram_transformer[cleaned_sentences_w]), labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
